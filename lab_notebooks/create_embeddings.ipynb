{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Create Embeddings for CDR3 Sequences (with specified model)\n",
    "##### Edit input and outputs paths. Input path should direct to the csv that has the sequences to create the embeddings for, and the output path should direct to the csv that will hold the embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/romi/projects/cvc\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 11:48:01.490444: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-03 11:48:01.492158: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-03 11:48:01.529620: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-03 11:48:01.530551: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-03 11:48:02.106657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "SRC_DIR = \"cvc\"\n",
    "assert os.path.isdir(SRC_DIR), f\"Cannot find src dir: {SRC_DIR}\"\n",
    "sys.path.append(SRC_DIR)\n",
    "\n",
    "from cvc import model_utils\n",
    "\n",
    "from lab_notebooks.utils import SC_TRANSFORMER, TRANSFORMER, DEVICE\n",
    "MODEL_DIR = os.path.join(SRC_DIR, \"models\")\n",
    "sys.path.append(MODEL_DIR)\n",
    "\n",
    "FILT_EDIT_DIST = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Specify Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# the sequences in the csv file need to be in a column called Sequences\n",
    "input_path = \"./CDR3_data/MAIT_cell_data_embeddings_8_datasets.csv\"\n",
    "output_path = \"./CDR3_data/MAIT_cell_data_embeddings_8_datasets_embeddings.csv\"\n",
    "label_column = \"MAIT_cell\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# specify which model to use\n",
    "# CVC - TRANSFORMER\n",
    "# scCVC - SC_TRANSFORMER\n",
    "TRANSFORMER_TO_USE = SC_TRANSFORMER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# load data\n",
    "tcrb_data = pd.read_csv(input_path, index_col=0)\n",
    "# drop duplicates\n",
    "tcrb_data = tcrb_data.drop_duplicates()\n",
    "# drop rows with NaN\n",
    "tcrb_data = tcrb_data.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              Sequences      MAIT_cell\n0                                        CAYRSVDSNYQLIW      MAIT_cell\n1                       CASSDREGEVSYNSPLHF|CASSFGGQPQHF      MAIT_cell\n2                                       CASSPREDSTDTQYF      MAIT_cell\n3                         CASSTGTGDGYTF|CAVDMDSNYQLIW||      MAIT_cell\n4             CASSYPGPTDTQYF|CAESLSDGQKLLF|CAVRDGDYKLSF      MAIT_cell\n...                                                 ...            ...\n6777   CAFMTNAGGTSYGKLTF|CASSQGAYGYTF|CAVAVGVSGGGADGLTF  non-MAIT_cell\n6778                             CASSYNEQFF|CAVETGNQFYF  non-MAIT_cell\n6779                  CASCSGTGYDEQYF|CASTGTSGGPTLRDEQFF  non-MAIT_cell\n6780  CASSVRSSMNTEAFF|CASSVVGGAGAYQETQYF|CASSVRSSMNT...  non-MAIT_cell\n6781               CAVIRWGDMRF|CASSPPGYYNEQFF|CASSLEVYF  non-MAIT_cell\n\n[6782 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sequences</th>\n      <th>MAIT_cell</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CAYRSVDSNYQLIW</td>\n      <td>MAIT_cell</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CASSDREGEVSYNSPLHF|CASSFGGQPQHF</td>\n      <td>MAIT_cell</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CASSPREDSTDTQYF</td>\n      <td>MAIT_cell</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CASSTGTGDGYTF|CAVDMDSNYQLIW||</td>\n      <td>MAIT_cell</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CASSYPGPTDTQYF|CAESLSDGQKLLF|CAVRDGDYKLSF</td>\n      <td>MAIT_cell</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6777</th>\n      <td>CAFMTNAGGTSYGKLTF|CASSQGAYGYTF|CAVAVGVSGGGADGLTF</td>\n      <td>non-MAIT_cell</td>\n    </tr>\n    <tr>\n      <th>6778</th>\n      <td>CASSYNEQFF|CAVETGNQFYF</td>\n      <td>non-MAIT_cell</td>\n    </tr>\n    <tr>\n      <th>6779</th>\n      <td>CASCSGTGYDEQYF|CASTGTSGGPTLRDEQFF</td>\n      <td>non-MAIT_cell</td>\n    </tr>\n    <tr>\n      <th>6780</th>\n      <td>CASSVRSSMNTEAFF|CASSVVGGAGAYQETQYF|CASSVRSSMNT...</td>\n      <td>non-MAIT_cell</td>\n    </tr>\n    <tr>\n      <th>6781</th>\n      <td>CAVIRWGDMRF|CASSPPGYYNEQFF|CASSLEVYF</td>\n      <td>non-MAIT_cell</td>\n    </tr>\n  </tbody>\n</table>\n<p>6782 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcrb_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# rename sequences column if not named \"Sequences\"\n",
    "tcrb_data.rename(columns={'cdr3': 'Sequences'}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/romi/projects/cvc/output_dir_singlecell_v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be51c09c58a448ba990ec5b7a1a4e643"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 180.00 MiB (GPU 0; 15.78 GiB total capacity; 1.55 GiB already allocated; 53.75 MiB free; 1.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcvc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membbeding_wrapper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EmbeddingWrapper\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Create embeddings\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m embed_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mEmbeddingWrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTRANSFORMER_TO_USE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtcrb_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmean\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpbar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m120\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m embed_wrap\u001B[38;5;241m.\u001B[39membeddings\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[0;32m~/projects/cvc/cvc/embbeding_wrapper.py:18\u001B[0m, in \u001B[0;36mEmbeddingWrapper.__init__\u001B[0;34m(self, model, device, sequences_df, batch_size, pbar, max_len, embeddings, **embedding_kwargs)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# if embeddings are not provided, create them\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embeddings \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 18\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_transformer_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpbar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpbar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43membedding_kwargs\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/cvc/cvc/model_utils.py:211\u001B[0m, in \u001B[0;36mget_transformer_embeddings\u001B[0;34m(model_dir, seqs, seq_pair, layers, method, batch_size, device, pbar, max_len)\u001B[0m\n\u001B[1;32m    209\u001B[0m encoded \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m encoded\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m    210\u001B[0m \u001B[38;5;66;03m# encoded contains input attention mask of (batch, seq_len)\u001B[39;00m\n\u001B[0;32m--> 211\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mencoded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m    213\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpool\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    216\u001B[0m     embeddings\u001B[38;5;241m.\u001B[39mappend(x\u001B[38;5;241m.\u001B[39mpooler_output\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat64))\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1019\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1010\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m   1012\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m   1013\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1014\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1017\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m   1018\u001B[0m )\n\u001B[0;32m-> 1019\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1020\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1027\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1028\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1030\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1031\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1032\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:609\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    600\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    601\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    602\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    606\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    607\u001B[0m     )\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 609\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    619\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    620\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:537\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    534\u001B[0m     cross_attn_present_key_value \u001B[38;5;241m=\u001B[39m cross_attention_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    535\u001B[0m     present_key_value \u001B[38;5;241m=\u001B[39m present_key_value \u001B[38;5;241m+\u001B[39m cross_attn_present_key_value\n\u001B[0;32m--> 537\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    538\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    540\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[1;32m    542\u001B[0m \u001B[38;5;66;03m# if decoder, return the attn key/values as the last output\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/transformers/pytorch_utils.py:249\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[1;32m    247\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[0;32m--> 249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:549\u001B[0m, in \u001B[0;36mBertLayer.feed_forward_chunk\u001B[0;34m(self, attention_output)\u001B[0m\n\u001B[1;32m    548\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[0;32m--> 549\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    550\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(intermediate_output, attention_output)\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:450\u001B[0m, in \u001B[0;36mBertIntermediate.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    449\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(hidden_states)\n\u001B[0;32m--> 450\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintermediate_act_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/envs/tcrbert/lib/python3.9/site-packages/transformers/activations.py:57\u001B[0m, in \u001B[0;36mGELUActivation.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 180.00 MiB (GPU 0; 15.78 GiB total capacity; 1.55 GiB already allocated; 53.75 MiB free; 1.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from cvc.embbeding_wrapper import EmbeddingWrapper\n",
    "\n",
    "# Create embeddings\n",
    "embed_wrap = EmbeddingWrapper(TRANSFORMER_TO_USE, DEVICE, tcrb_data, batch_size=256, method=\"mean\", layers=[-1], pbar=True, max_len=120)\n",
    "embed_wrap.embeddings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1         2         3         4         5         6    \\\n0    -0.149587 -0.197047 -0.606274  0.246161  0.146642 -0.208196  0.247288   \n1    -0.003837  0.004939 -0.305185 -0.467737  0.554620  0.526504  0.037872   \n2     0.027054  0.238266  0.138490 -0.477724  0.421007  0.106373 -0.682291   \n3    -0.149578  0.100374 -0.122322 -0.478098  0.454484  0.019130  0.048994   \n4    -0.540540  0.291595 -0.353379 -0.261123  0.426946  0.357447 -0.488219   \n...        ...       ...       ...       ...       ...       ...       ...   \n5011 -0.329775  0.793744 -1.056300  0.181084  0.043187 -0.084797  0.497528   \n5012 -0.100326  0.304981  0.190182 -0.059698  0.107292  0.121101 -0.251405   \n5013 -0.056023 -0.094397 -0.433103 -0.062313  0.069507 -0.040174  0.057963   \n5014 -0.422617  0.676036 -1.092209  0.034845  0.449366  0.100408  0.365332   \n5015 -0.679665  0.381582 -0.096865 -0.755159  0.349162  0.068600 -0.496824   \n\n           7         8         9    ...       758       759       760  \\\n0    -0.079729  0.198015 -0.245093  ...  0.221753  0.178292 -0.638298   \n1    -0.058776  0.078017 -0.378901  ... -0.394406 -0.488040  0.325271   \n2    -0.285417  0.483916 -0.346199  ... -0.040932 -0.713956  0.507948   \n3     0.235329  0.132402 -0.505863  ... -0.385887 -0.625642  0.282709   \n4    -0.435053  0.388957 -0.267924  ...  0.051595 -0.938021  0.328469   \n...        ...       ...       ...  ...       ...       ...       ...   \n5011  0.468565  0.060517  0.038454  ...  0.523287 -0.355889 -0.269269   \n5012 -0.306332  0.125169 -0.237075  ...  0.001577 -0.410296 -0.172579   \n5013 -0.075291  0.027573 -0.167322  ... -0.025749 -0.564122 -0.157909   \n5014  0.275192  0.020950  0.030260  ...  0.410969  0.098768 -0.000810   \n5015 -0.169834  0.225958 -0.460315  ... -0.274555 -0.882894 -0.035289   \n\n           761       762       763       764       765       766       767  \n0     0.011018 -0.319213  0.780746 -0.412318  0.522235  0.514297  0.207887  \n1    -0.620283 -0.451306 -0.204477 -0.042363  0.488556  0.090419  0.102915  \n2    -0.131854  0.230915  0.139384 -0.132491  0.416187  0.232041 -0.580679  \n3     0.410364  0.008110 -0.087051 -0.256943  0.420582 -0.317372  0.073733  \n4    -0.103942  0.323352  0.735837  0.272375  0.473237  0.894415 -0.243902  \n...        ...       ...       ...       ...       ...       ...       ...  \n5011 -0.119622 -0.516974  0.407369  0.187545  0.344213  0.109828  0.269770  \n5012 -0.399773 -0.378466  0.207969 -0.128755  0.727024  0.299680  0.027128  \n5013  0.064306 -0.047353  0.050179 -0.237840  0.765852 -0.496809  0.229118  \n5014 -0.096166 -0.774795  0.810294 -0.045002  0.009765  0.297485  0.518775  \n5015 -0.299447 -0.159665  0.281305 -0.450529  0.594555  0.529679 -0.038543  \n\n[5016 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.149587</td>\n      <td>-0.197047</td>\n      <td>-0.606274</td>\n      <td>0.246161</td>\n      <td>0.146642</td>\n      <td>-0.208196</td>\n      <td>0.247288</td>\n      <td>-0.079729</td>\n      <td>0.198015</td>\n      <td>-0.245093</td>\n      <td>...</td>\n      <td>0.221753</td>\n      <td>0.178292</td>\n      <td>-0.638298</td>\n      <td>0.011018</td>\n      <td>-0.319213</td>\n      <td>0.780746</td>\n      <td>-0.412318</td>\n      <td>0.522235</td>\n      <td>0.514297</td>\n      <td>0.207887</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.003837</td>\n      <td>0.004939</td>\n      <td>-0.305185</td>\n      <td>-0.467737</td>\n      <td>0.554620</td>\n      <td>0.526504</td>\n      <td>0.037872</td>\n      <td>-0.058776</td>\n      <td>0.078017</td>\n      <td>-0.378901</td>\n      <td>...</td>\n      <td>-0.394406</td>\n      <td>-0.488040</td>\n      <td>0.325271</td>\n      <td>-0.620283</td>\n      <td>-0.451306</td>\n      <td>-0.204477</td>\n      <td>-0.042363</td>\n      <td>0.488556</td>\n      <td>0.090419</td>\n      <td>0.102915</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.027054</td>\n      <td>0.238266</td>\n      <td>0.138490</td>\n      <td>-0.477724</td>\n      <td>0.421007</td>\n      <td>0.106373</td>\n      <td>-0.682291</td>\n      <td>-0.285417</td>\n      <td>0.483916</td>\n      <td>-0.346199</td>\n      <td>...</td>\n      <td>-0.040932</td>\n      <td>-0.713956</td>\n      <td>0.507948</td>\n      <td>-0.131854</td>\n      <td>0.230915</td>\n      <td>0.139384</td>\n      <td>-0.132491</td>\n      <td>0.416187</td>\n      <td>0.232041</td>\n      <td>-0.580679</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.149578</td>\n      <td>0.100374</td>\n      <td>-0.122322</td>\n      <td>-0.478098</td>\n      <td>0.454484</td>\n      <td>0.019130</td>\n      <td>0.048994</td>\n      <td>0.235329</td>\n      <td>0.132402</td>\n      <td>-0.505863</td>\n      <td>...</td>\n      <td>-0.385887</td>\n      <td>-0.625642</td>\n      <td>0.282709</td>\n      <td>0.410364</td>\n      <td>0.008110</td>\n      <td>-0.087051</td>\n      <td>-0.256943</td>\n      <td>0.420582</td>\n      <td>-0.317372</td>\n      <td>0.073733</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.540540</td>\n      <td>0.291595</td>\n      <td>-0.353379</td>\n      <td>-0.261123</td>\n      <td>0.426946</td>\n      <td>0.357447</td>\n      <td>-0.488219</td>\n      <td>-0.435053</td>\n      <td>0.388957</td>\n      <td>-0.267924</td>\n      <td>...</td>\n      <td>0.051595</td>\n      <td>-0.938021</td>\n      <td>0.328469</td>\n      <td>-0.103942</td>\n      <td>0.323352</td>\n      <td>0.735837</td>\n      <td>0.272375</td>\n      <td>0.473237</td>\n      <td>0.894415</td>\n      <td>-0.243902</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5011</th>\n      <td>-0.329775</td>\n      <td>0.793744</td>\n      <td>-1.056300</td>\n      <td>0.181084</td>\n      <td>0.043187</td>\n      <td>-0.084797</td>\n      <td>0.497528</td>\n      <td>0.468565</td>\n      <td>0.060517</td>\n      <td>0.038454</td>\n      <td>...</td>\n      <td>0.523287</td>\n      <td>-0.355889</td>\n      <td>-0.269269</td>\n      <td>-0.119622</td>\n      <td>-0.516974</td>\n      <td>0.407369</td>\n      <td>0.187545</td>\n      <td>0.344213</td>\n      <td>0.109828</td>\n      <td>0.269770</td>\n    </tr>\n    <tr>\n      <th>5012</th>\n      <td>-0.100326</td>\n      <td>0.304981</td>\n      <td>0.190182</td>\n      <td>-0.059698</td>\n      <td>0.107292</td>\n      <td>0.121101</td>\n      <td>-0.251405</td>\n      <td>-0.306332</td>\n      <td>0.125169</td>\n      <td>-0.237075</td>\n      <td>...</td>\n      <td>0.001577</td>\n      <td>-0.410296</td>\n      <td>-0.172579</td>\n      <td>-0.399773</td>\n      <td>-0.378466</td>\n      <td>0.207969</td>\n      <td>-0.128755</td>\n      <td>0.727024</td>\n      <td>0.299680</td>\n      <td>0.027128</td>\n    </tr>\n    <tr>\n      <th>5013</th>\n      <td>-0.056023</td>\n      <td>-0.094397</td>\n      <td>-0.433103</td>\n      <td>-0.062313</td>\n      <td>0.069507</td>\n      <td>-0.040174</td>\n      <td>0.057963</td>\n      <td>-0.075291</td>\n      <td>0.027573</td>\n      <td>-0.167322</td>\n      <td>...</td>\n      <td>-0.025749</td>\n      <td>-0.564122</td>\n      <td>-0.157909</td>\n      <td>0.064306</td>\n      <td>-0.047353</td>\n      <td>0.050179</td>\n      <td>-0.237840</td>\n      <td>0.765852</td>\n      <td>-0.496809</td>\n      <td>0.229118</td>\n    </tr>\n    <tr>\n      <th>5014</th>\n      <td>-0.422617</td>\n      <td>0.676036</td>\n      <td>-1.092209</td>\n      <td>0.034845</td>\n      <td>0.449366</td>\n      <td>0.100408</td>\n      <td>0.365332</td>\n      <td>0.275192</td>\n      <td>0.020950</td>\n      <td>0.030260</td>\n      <td>...</td>\n      <td>0.410969</td>\n      <td>0.098768</td>\n      <td>-0.000810</td>\n      <td>-0.096166</td>\n      <td>-0.774795</td>\n      <td>0.810294</td>\n      <td>-0.045002</td>\n      <td>0.009765</td>\n      <td>0.297485</td>\n      <td>0.518775</td>\n    </tr>\n    <tr>\n      <th>5015</th>\n      <td>-0.679665</td>\n      <td>0.381582</td>\n      <td>-0.096865</td>\n      <td>-0.755159</td>\n      <td>0.349162</td>\n      <td>0.068600</td>\n      <td>-0.496824</td>\n      <td>-0.169834</td>\n      <td>0.225958</td>\n      <td>-0.460315</td>\n      <td>...</td>\n      <td>-0.274555</td>\n      <td>-0.882894</td>\n      <td>-0.035289</td>\n      <td>-0.299447</td>\n      <td>-0.159665</td>\n      <td>0.281305</td>\n      <td>-0.450529</td>\n      <td>0.594555</td>\n      <td>0.529679</td>\n      <td>-0.038543</td>\n    </tr>\n  </tbody>\n</table>\n<p>5016 rows × 768 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcrb_embeddings_df = pd.DataFrame(embed_wrap.embeddings)\n",
    "tcrb_embeddings_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create anndata object\n",
    "tcrb_embeddings_adata = embed_wrap.create_anndata()\n",
    "\n",
    "# add public/private label to dataframe\n",
    "df_embeddings_with_label = tcrb_embeddings_df\n",
    "df_embeddings_with_label['Sequences']=list(tcrb_embeddings_adata.obs['Sequences'])\n",
    "df_embeddings_with_label[label_column]=list(tcrb_embeddings_adata.obs[label_column])\n",
    "df_embeddings_with_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "OUTPUT_FORMAT = \"pickle\" # csv or pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'MAIT_cell' as categorical\n"
     ]
    }
   ],
   "source": [
    "# output embeddings to csv\n",
    "if OUTPUT_FORMAT == \"csv\":\n",
    "    tcrb_embeddings_df.to_csv(output_path, index=False)\n",
    "elif OUTPUT_FORMAT == \"pickle\":\n",
    "    import pickle\n",
    "    with open(output_path, 'wb') as handle:\n",
    "        pickle.dump(tcrb_embeddings_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# save anndata object\n",
    "tcrb_embeddings_adata.write_h5ad(output_path.replace(\".csv\", \".h5ad\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cef408d8-9130-407c-bfc8-1126d6b70d18",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
